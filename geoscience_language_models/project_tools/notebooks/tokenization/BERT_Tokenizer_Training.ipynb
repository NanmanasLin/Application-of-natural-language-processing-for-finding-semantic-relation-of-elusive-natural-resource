{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "third-insured",
   "metadata": {},
   "source": [
    "# BERT Tokenizer Training\n",
    "Copyright (C) 2021 ServiceNow, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-brick",
   "metadata": {},
   "source": [
    "Notebook space for testing for a tokenizer with geo domain data.  \n",
    "\n",
    "Notebook can be run from top to bottom, but sections also generally run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "analyzed-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-fleece",
   "metadata": {},
   "source": [
    "## Test script for tokenizer training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-pierre",
   "metadata": {},
   "source": [
    "Add the project directory to the path to allow for importing modules from the nrcan directory. \n",
    "\n",
    "On toolkit, this should print something like `/nrcan_p2/workspace/<USERNAME>/nrcan_p2`, where the key piece is the last `/nrcan_p2` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "toxic-offer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nrcan_p2/workspace/lindsay/nrcan_p2\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "REPO_DIR = pathlib.Path(__name__).parent.absolute().parent.parent\n",
    "print(REPO_DIR)\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(REPO_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "external-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrcan_p2.tokenization.custom_tokenizer import (train_WordPiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "artificial-diabetes",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_WordPiece(input_files = '/nrcan_p2/data/03_primary/v1/short_text_5M.txt', \n",
    "#                 save_path = '/nrcan_p2/data/06_models/tokenizers/geo_trained/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-facing",
   "metadata": {},
   "source": [
    "The above cell is commented out because (a) the input file doesn't exist anymore, and (b) this was for testing, and this action is now done through a separate script. If it were run, it would output the following:\n",
    "```\n",
    "Argument `input_files` given as string; converting to list.\n",
    "Input file exists: /nrcan_p2/data/03_primary/v1/short_text_5M.txt\n",
    "Output directory exists: /nrcan_p2/data/06_models/tokenizers/geo_trained/\n",
    "Model will be saved with filenamewordpiece_geo_short_text_5M_2021-02-03_14:46:34\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-shepherd",
   "metadata": {},
   "source": [
    "## Scratchpad for developing tokenizer training script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-series",
   "metadata": {},
   "source": [
    "This section includes logic used in development of `run_tokenizer_training.py` and `custom_tokenizer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "color-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alike-congo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-03-12_14:37:11'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now(tz=None).strftime(\"%Y-%m-%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bacterial-amendment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wordpiece_geo_thispart'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"wordpiece_geo_\" + \\\n",
    "        os.path.splitext('safd/asfad/thispart.txt'.rsplit(sep='/', maxsplit=1)[-1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bridal-state",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory exists: /nrcan_p2/data/06_models/tokenizers/geo_trained/\n"
     ]
    }
   ],
   "source": [
    "string = \"/nrcan_p2/data/06_models/tokenizers/geo_trained/\"\n",
    "\n",
    "import sys\n",
    "if os.path.isdir(string):\n",
    "    print('Output directory exists: ' + string)\n",
    "else:\n",
    "    print('Output directory does not exist: ' + string)\n",
    "    sys.exit('Output directory not found: ' + string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "isolated-forth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nrcan_p2/data/06_models/tokenizers/geo_trained/tokenizername_otherinfo.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_name = \"/nrcan_p2/data/06_models/tokenizers/geo_trained/\"\n",
    "base_filename = 'tokenizername'\n",
    "\n",
    "os.path.join(dir_name, base_filename + \"_otherinfo\" + '.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "modified-completion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please give input_files as list.\n"
     ]
    }
   ],
   "source": [
    "input_files = \"/nrcan_p2/data/06_models/tokenizers/geo_trained/\"\n",
    "\n",
    "if not(isinstance(input_files, list)):\n",
    "    print('Please give input_files as list.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "considerable-thriller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/nrcan_p2/data/06_models/tokenizers/geo_trained/']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_files = \"/nrcan_p2/data/06_models/tokenizers/geo_trained/\"\n",
    "\n",
    "if isinstance(input_files, str):\n",
    "    input_files = [input_files]\n",
    "\n",
    "print(input_files)\n",
    "isinstance(input_files, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "independent-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all_text_SIMPLE_PIPELINE_BERT_3_POSTPIPE_BERT_SPACY_2_dB_v1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_files = [\"/nrcan_p2/data/04_feature/v4_B/all_text_SIMPLE_PIPELINE_BERT_3_POSTPIPE_BERT_SPACY_2_dB_v1/train.txt\"]\n",
    "# input_files = [\"/nrcan_p2/data/03_primary/v1/short_text_5M.txt\"]\n",
    "\n",
    "filename_base = os.path.splitext(input_files[0].rsplit(sep='/', maxsplit=1)[-1])[0]\n",
    "if filename_base == 'train':\n",
    "    filename_base = os.path.splitext(input_files[0].rsplit(sep='/', maxsplit=2)[-2])[0]\n",
    "filename_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "opened-hughes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting at: 2021-03-12 14:37:12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training starting at: {datetime.now(tz=None).strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-setting",
   "metadata": {},
   "source": [
    "## Add new tokens to pretrained tokenizer using `add_tokens`, and examine behaviour (spoiler alert: it's inconsistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "complimentary-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from tokenizers import Tokenizer, AddedToken\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "plastic-colonial",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in standard tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "frank-phase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sidenote: Do BERT and DistilBERT tokenizers have the same vocabulary?\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer.vocab.keys() == tokenizer_bert.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pending-cologne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"geology\": True\n",
      "\"##ozoic\": False\n",
      "\"meso\": False\n",
      "\"america\": True\n",
      "\"paleo\": False\n",
      "\"ization\": False\n",
      "\"##ization\": True\n",
      "\"##tation\": True\n",
      "\"##ceous\": False\n",
      "\"paleontologist\": False\n"
     ]
    }
   ],
   "source": [
    "# Check for some tokens in tokenizer\n",
    "for my_token in [\"geology\", \n",
    "                 \"##ozoic\", \n",
    "                 \"meso\", \n",
    "                 \"america\",\n",
    "                 \"paleo\", \n",
    "                 \"ization\", \"##ization\", \n",
    "                 \"##tation\", \n",
    "                 \"##ceous\",\n",
    "                 \"paleontologist\"]:\n",
    "    print(f'\"{my_token}\": {my_token in tokenizer.vocab.keys() | tokenizer.added_tokens_encoder.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daily-particular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pale',\n",
       " '##oz',\n",
       " '##oic',\n",
       " 'me',\n",
       " '##so',\n",
       " '##ame',\n",
       " '##rica',\n",
       " 'me',\n",
       " '##ame',\n",
       " '##rica',\n",
       " 'crust',\n",
       " '##aceous',\n",
       " 'pale',\n",
       " '##ont',\n",
       " '##ologist']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"paleozoic mesoamerica meamerica crustaceous paleontologist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "quarterly-floor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mental-involvement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new tokens to tokenizer\n",
    "new_vocab = [AddedToken(\"ceous\", single_word=False), \n",
    "             AddedToken(\"meso\", single_word=False), \n",
    "             AddedToken(\"paleo\", single_word=False),\n",
    "             AddedToken(\"zoic\", single_word=False),\n",
    "             AddedToken(\"paleontologist\", single_word=True)]\n",
    "tokenizer.add_tokens(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sporting-tournament",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30527"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "experienced-accent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"geology\": True\n",
      "\"##ozoic\": False\n",
      "\"meso\": True\n",
      "\"america\": True\n",
      "\"paleo\": True\n",
      "\"ization\": False\n",
      "\"##ization\": True\n",
      "\"##tation\": True\n",
      "\"##ceous\": False\n",
      "\"paleontologist\": True\n"
     ]
    }
   ],
   "source": [
    "# Check for some tokens in tokenizer\n",
    "for my_token in [\"geology\", \n",
    "                 \"##ozoic\", \n",
    "                 \"meso\", \n",
    "                 \"america\",\n",
    "                 \"paleo\", \n",
    "                 \"ization\", \"##ization\", \n",
    "                 \"##tation\", \n",
    "                 \"##ceous\",\n",
    "                 \"paleontologist\"]:\n",
    "    print(f'\"{my_token}\": {my_token in tokenizer.vocab.keys()| tokenizer.added_tokens_encoder.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "trying-victoria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paleo',\n",
       " 'zoic',\n",
       " 'meso',\n",
       " 'america',\n",
       " 'me',\n",
       " '##ame',\n",
       " '##rica',\n",
       " 'crust',\n",
       " '##a',\n",
       " 'ceous',\n",
       " 'paleontologist']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"paleozoic mesoamerica meamerica crustaceous paleontologist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fatty-burden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meso', 'america', 'me', '##ame', '##rica', 'meso', 'america']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"mesoamerica meamerica mesoamerica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "false-standard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meso', 'america']\n",
      "['table', '##ame', '##rica']\n",
      "['america']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"mesoamerica\"))\n",
    "print(tokenizer.tokenize(\"tableamerica\"))\n",
    "print(tokenizer.tokenize(\"america\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-moscow",
   "metadata": {},
   "source": [
    "* After an added token, the rest of the word is treated as a separate whole word if possible (greedy).\n",
    "* After a regular token that is a whole word, the rest of the word is tokenized into sub-words.\n",
    "\n",
    "For posterity, to demonstrate, the cell above:\n",
    "```\n",
    "print(tokenizer.tokenize(\"mesoamerica\"))\n",
    "print(tokenizer.tokenize(\"tableamerica\"))\n",
    "print(tokenizer.tokenize(\"america\"))\n",
    "```\n",
    "\n",
    "gives the following output:\n",
    "\n",
    "```\n",
    "['meso', 'america']\n",
    "['table', '##ame', '##rica']\n",
    "['america']\n",
    "```\n",
    "\n",
    "where `meso` is an added token, and `table` was present in the pretrained tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-punishment",
   "metadata": {},
   "source": [
    "## Code for saving and re-reading tokenizer, with original and modified `vocab.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-dealer",
   "metadata": {},
   "source": [
    "This approach leads to expected tokenizer behaviour, in contrast with the `add_tokens` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "stunning-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "amber-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "banner-mongolia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/nrcan_p2/data/06_models/tokenizers/testing/generic/tokenizer_config.json',\n",
       " '/nrcan_p2/data/06_models/tokenizers/testing/generic/special_tokens_map.json',\n",
       " '/nrcan_p2/data/06_models/tokenizers/testing/generic/vocab.txt',\n",
       " '/nrcan_p2/data/06_models/tokenizers/testing/generic/added_tokens.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('/nrcan_p2/data/06_models/tokenizers/testing/generic/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "structured-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_read = BertTokenizer.from_pretrained('/nrcan_p2/data/06_models/tokenizers/testing/generic/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-asset",
   "metadata": {},
   "source": [
    "Test tokenization with original tokenizer (saved and read back in):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "referenced-edward",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pale',\n",
       " '##oz',\n",
       " '##oic',\n",
       " 'pale',\n",
       " '##ont',\n",
       " '##ologist',\n",
       " 'me',\n",
       " '##so',\n",
       " '##ame',\n",
       " '##rica',\n",
       " 'me',\n",
       " '##so',\n",
       " '##zo',\n",
       " '##ic',\n",
       " 'america']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization with original tokenizer\n",
    "tokenizer_read.tokenize(\"paleozoic paleontologist mesoamerica mesozoic america\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-pepper",
   "metadata": {},
   "source": [
    "Copy manually modified vocab to model directory `vocab.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "english-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /nrcan_p2/data/06_models/tokenizers/testing/vocab_lists/vocab_edited.txt /nrcan_p2/data/06_models/tokenizers/testing/generic/vocab.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-transition",
   "metadata": {},
   "source": [
    "This added the following tokens to the vocabulary:\n",
    "- `##zoic`\n",
    "- `paleontologist`\n",
    "- `meso`\n",
    "- `paleo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-welding",
   "metadata": {},
   "source": [
    "Read model in (including modified `vocab.txt`) and test tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "delayed-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_read = BertTokenizer.from_pretrained('/nrcan_p2/data/06_models/tokenizers/testing/generic/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "occupied-creator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paleo',\n",
       " '##zoic',\n",
       " 'paleontologist',\n",
       " 'meso',\n",
       " '##ame',\n",
       " '##rica',\n",
       " 'meso',\n",
       " '##zoic',\n",
       " 'america']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_read.tokenize(\"paleozoic paleontologist mesoamerica mesozoic america\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-detector",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-tomorrow",
   "metadata": {},
   "source": [
    "## Geo tokenizer, analysis of new tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-sleeve",
   "metadata": {},
   "source": [
    "### Original tokenizer vocab (full datasets)\n",
    "\n",
    "_The conclusion of this section is that there are too many messy tokens for this to be useful without substantial manual input into selecting new tokens. To address this, we created a new dataset (very clean but much smaller) using document metadata, and used that to train the geology tokenizer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "prime-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-motorcycle",
   "metadata": {},
   "source": [
    "Get vocab from generic tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "described-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-respondent",
   "metadata": {},
   "source": [
    "Standard BERT tokenizer vocab list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "extra-comedy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = list(tokenizer.vocab.keys())\n",
    "len(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-industry",
   "metadata": {},
   "source": [
    "Get vocab from geo tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "involved-topic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'wordpiece_geo_short_text_5M_2021-02-03_21:55:51-vocab.txt',\n",
       " 'wordpiece_geo_short_text_5M_2021-02-03_21:55:51.json',\n",
       " 'wordpiece_geo_all_text_SIMPLE_PIPELINE_BERT_3_POSTPIPE_BERT_SPACY_2_dA_full_v1_20210209_004618-vocab.txt',\n",
       " 'wordpiece_geo_all_text_SIMPLE_PIPELINE_BERT_3_POSTPIPE_BERT_SPACY_2_dA_full_v1_20210209_004618.json',\n",
       " 'wordpiece_geo_all_text_SIMPLE_PIPELINE_BERT_3_POSTPIPE_BERT_SPACY_2_dB_v1_20210209_004703-vocab.txt',\n",
       " 'wordpiece_geo_all_text_SIMPLE_PIPELINE_BERT_3_POSTPIPE_BERT_SPACY_2_dB_v1_20210209_004703.json',\n",
       " 'wordpiece_geo_all_text_SIMPLE_PIPELINE_BERT_3_POSTPIPE_BERT_SPACY_2_dA_full_dB_v1_20210209_004417-vocab.txt',\n",
       " 'wordpiece_geo_all_text_SIMPLE_PIPELINE_BERT_3_POSTPIPE_BERT_SPACY_2_dA_full_dB_v1_20210209_004417.json',\n",
       " 'tokens_head.txt',\n",
       " 'wordpiece_geo_EAIDown.xml_processed_nosentences_20210223_161710-vocab.txt',\n",
       " 'wordpiece_geo_EAIDown.xml_processed_nosentences_20210223_161710.json']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/nrcan_p2/data/06_models/tokenizers/geo_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "stopped-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join('/nrcan_p2/data/06_models/tokenizers/geo_trained', \n",
    "                          'wordpiece_geo_all_text_SIMPLE_PIPELINE_BERT_3_POSTPIPE_BERT_SPACY_2_dA_full_v1_20210209_004618-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "filled-allergy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30517</th>\n",
       "      <td>085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30518</th>\n",
       "      <td>beset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30519</th>\n",
       "      <td>nipis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30520</th>\n",
       "      <td>traditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30521</th>\n",
       "      <td>californie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30522 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            token\n",
       "0           [UNK]\n",
       "1           [CLS]\n",
       "2           [SEP]\n",
       "3           [PAD]\n",
       "4          [MASK]\n",
       "...           ...\n",
       "30517         085\n",
       "30518       beset\n",
       "30519       nipis\n",
       "30520  traditions\n",
       "30521  californie\n",
       "\n",
       "[30522 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df = pd.read_csv(vocab_file, sep=\"\\n\", header=None)\n",
    "geo_df.rename(columns = {0:'token'}, inplace = True)\n",
    "geo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "static-painting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000th token: ##rd\n"
     ]
    }
   ],
   "source": [
    "print(f'5000th token: {geo_df.iloc[5000,0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cutting-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_file) as f:\n",
    "    temp = f.read()\n",
    "    geo = temp.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "downtown-crash",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens: 30522\n",
      "5000th token: ##rd\n"
     ]
    }
   ],
   "source": [
    "print(f'number of tokens: {len(geo)}')\n",
    "print(f'5000th token: {geo[5000]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "derived-crazy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17686"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_new = list(set(geo).difference(set(bert)))\n",
    "len(geo_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "severe-evidence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "km is new? False\n",
      "howe is new? False\n",
      "##ott is new? False\n",
      "limited is new? False\n",
      "brown is new? False\n",
      "side is new? False\n",
      "represent is new? False\n",
      "many is new? False\n",
      "##umm is new? True\n",
      "fo is new? True\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "for n in list(range(1000,1010)):\n",
    "    print(f'{geo_df.iloc[n,0]} is new? {geo_df.iloc[n,0] in list(geo_new)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "positive-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df['is_new'] = [geo_df['token'][i] in list(geo_new) for i in range(0,len(geo_df['token']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "chief-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df['sub_word'] = ['##' in str(geo_df['token'][i]) for i in range(0,len(geo_df['token']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "forced-vietnam",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>is_new</th>\n",
       "      <th>sub_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>wh</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>canad</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>##pos</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>##olog</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>.,</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30516</th>\n",
       "      <td>urss</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30517</th>\n",
       "      <td>085</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30518</th>\n",
       "      <td>beset</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30519</th>\n",
       "      <td>nipis</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30521</th>\n",
       "      <td>californie</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17686 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            token  is_new  sub_word\n",
       "195            wh    True     False\n",
       "234         canad    True     False\n",
       "239         ##pos    True      True\n",
       "240        ##olog    True      True\n",
       "254            .,    True     False\n",
       "...           ...     ...       ...\n",
       "30516        urss    True     False\n",
       "30517         085    True     False\n",
       "30518       beset    True     False\n",
       "30519       nipis    True     False\n",
       "30521  californie    True     False\n",
       "\n",
       "[17686 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df[geo_df['is_new']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-tender",
   "metadata": {},
   "source": [
    "I want to check if something is a full English word or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-chapel",
   "metadata": {},
   "source": [
    "I tested `langdetect`, which was not helpful; output is in markdown below to preserve learnings even though the module is not still installed in this container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "flying-couple",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !poetry config virtualenvs.create false; poetry add langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "identical-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langdetect import detect as detect_lang\n",
    "# from langdetect import detect_langs\n",
    "# detect_lang(string) <-- should return a string\n",
    "# detect_langs(string) <-- should return a list of lang objects (lang.prob, lang.lang to get the probability and the language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "outer-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langdetect import detect as detect_lang\n",
    "# from langdetect import detect_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "retained-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(300,310):\n",
    "#     print(geo_df['token'][i] + ': ' + detect_lang(geo_df['token'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-atmosphere",
   "metadata": {},
   "source": [
    "Output:\n",
    "```\n",
    "au: fr\n",
    "per: id\n",
    "##ont: fi\n",
    "riv: it\n",
    "inter: nl\n",
    "which: en\n",
    "##ence: es\n",
    "##iz: hr\n",
    "no: tl\n",
    "##ost: fi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "packed-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect_langs('canada')\n",
    "# detect_langs('canad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-comfort",
   "metadata": {},
   "source": [
    "Output: \n",
    "```\n",
    "[so:0.5714273198131568, es:0.42857250393263935]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-medium",
   "metadata": {},
   "source": [
    "`langdetect` doesn't work as desired. It predicts which language something is, and sometimes predicts that single English words are other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-reverse",
   "metadata": {},
   "source": [
    "Further explore new tokens. Note that output is reduced for ease of reading; remove `.head(20)` to see full data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "traditional-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "printable-italy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>is_new</th>\n",
       "      <th>sub_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>wh</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>canad</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>.,</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>comp</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>gra</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>geolog</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>gre</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>sou</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>riv</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>cont</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>pres</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>dist</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>produ</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>comm</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>occ</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>cons</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>loc</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>str</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>dep</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>spe</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token  is_new  sub_word\n",
       "195      wh    True     False\n",
       "234   canad    True     False\n",
       "254      .,    True     False\n",
       "271    comp    True     False\n",
       "275     gra    True     False\n",
       "292  geolog    True     False\n",
       "294     gre    True     False\n",
       "296     sou    True     False\n",
       "303     riv    True     False\n",
       "316    cont    True     False\n",
       "332    pres    True     False\n",
       "356    dist    True     False\n",
       "358   produ    True     False\n",
       "383    comm    True     False\n",
       "390     occ    True     False\n",
       "393    cons    True     False\n",
       "404     loc    True     False\n",
       "416     str    True     False\n",
       "421     dep    True     False\n",
       "424     spe    True     False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df[(geo_df['is_new']==True) & (geo_df['sub_word']==False)].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "maritime-friendly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>is_new</th>\n",
       "      <th>sub_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>##pos</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>##olog</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>##iqu</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>##posit</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>##orm</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>##iment</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>##erv</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>##ediment</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>##ould</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>##cess</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>##oup</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>##rib</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>##ork</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>##ograph</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>##laci</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>##ween</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>##oci</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>##ruct</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>##iles</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>##ature</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token  is_new  sub_word\n",
       "239      ##pos    True      True\n",
       "240     ##olog    True      True\n",
       "286      ##iqu    True      True\n",
       "331    ##posit    True      True\n",
       "378      ##orm    True      True\n",
       "419    ##iment    True      True\n",
       "450      ##erv    True      True\n",
       "467  ##ediment    True      True\n",
       "500     ##ould    True      True\n",
       "502     ##cess    True      True\n",
       "504      ##oup    True      True\n",
       "515      ##rib    True      True\n",
       "523      ##ork    True      True\n",
       "554   ##ograph    True      True\n",
       "570     ##laci    True      True\n",
       "582     ##ween    True      True\n",
       "593      ##oci    True      True\n",
       "619     ##ruct    True      True\n",
       "622     ##iles    True      True\n",
       "631    ##ature    True      True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df[(geo_df['is_new']==True) & (geo_df['sub_word']==True)].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "manual-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert: False   geo: True     meso\n",
      "bert: False   geo: True     paleo\n",
      "bert: True   geo: True     ##aceous\n",
      "bert: False   geo: True     ##ceous\n",
      "bert: False   geo: False     ##ceou\n",
      "bert: True   geo: False     ##eous\n",
      "bert: False   geo: True     ##ozoic\n",
      "bert: False   geo: False     ##zoic\n",
      "bert: False   geo: False     ##ozoi\n"
     ]
    }
   ],
   "source": [
    "for i in ['meso', 'paleo', \n",
    "          '##aceous', '##ceous', '##ceou', '##eous', \n",
    "          '##ozoic', '##zoic', \"##ozoi\"]:\n",
    "    print(f'bert: {i in bert}   geo: {i in geo}     {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ordinary-christmas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert: False   geo: True     meso\n",
      "bert: False   geo: True     paleo\n",
      "bert: False   geo: True     aceous\n",
      "bert: False   geo: True     ceous\n",
      "bert: False   geo: False     ceou\n",
      "bert: False   geo: False     eous\n",
      "bert: False   geo: False     ozoic\n",
      "bert: False   geo: False     zoic\n",
      "bert: False   geo: False     ozoi\n"
     ]
    }
   ],
   "source": [
    "for i in ['meso', 'paleo', \n",
    "          'aceous', 'ceous', 'ceou', 'eous', \n",
    "          'ozoic', 'zoic', \"ozoi\"]:\n",
    "    print(f'bert: {i in bert}   geo: {i in geo}     {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "steady-audience",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>is_new</th>\n",
       "      <th>sub_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>##ceous</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>##ozoic</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>##aceous</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20600</th>\n",
       "      <td>ceous</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23916</th>\n",
       "      <td>aceous</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  is_new  sub_word\n",
       "1148    ##ceous    True      True\n",
       "1162    ##ozoic    True      True\n",
       "2469   ##aceous   False      True\n",
       "20600     ceous    True     False\n",
       "23916    aceous    True     False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df[geo_df['token'].isin(['aceous', 'ceous', 'ceou', 'eous', \n",
    "                             'ozoic', 'zoic', \"ozoi\",\n",
    "                             '##aceous', '##ceous', '##ceou', '##eous', \n",
    "                             '##ozoic', '##zoic', \"##ozoi\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-campbell",
   "metadata": {},
   "source": [
    "---\n",
    "### New, cleaner tokenizer vocab (trained on metadata only)\n",
    "\n",
    "_Tokenizer training on this new dataset produced much cleaner tokens, and this is what we used to create the geology-specific tokenizer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "interesting-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-castle",
   "metadata": {},
   "source": [
    "Get vocab from generic BERT tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "complicated-vancouver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "bert = list(tokenizer.vocab.keys())\n",
    "len(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-capability",
   "metadata": {},
   "source": [
    "Get vocab from shiny new, clean geo tokenizer (on small data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "institutional-rover",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wordpiece_geo_EAIDown.xml_processed_nosentences_20210223_161710-vocab.txt',\n",
       " 'wordpiece_geo_EAIDown.xml_processed_nosentences_20210223_161710.json']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in os.listdir('/nrcan_p2/data/06_models/tokenizers/geo_trained') if 'nosentences' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "headed-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join('/nrcan_p2/data/06_models/tokenizers/geo_trained', \n",
    "                          'wordpiece_geo_EAIDown.xml_processed_nosentences_20210223_161710-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "sharp-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_file) as f:\n",
    "    temp = f.read()\n",
    "    geo = temp.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "worthy-thing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token\n",
       "0   [UNK]\n",
       "1   [CLS]\n",
       "2   [SEP]\n",
       "3   [PAD]\n",
       "4  [MASK]\n",
       "5       !\n",
       "6       \"\n",
       "7       #\n",
       "8       $\n",
       "9       %"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df = pd.DataFrame.from_dict({'token': geo})\n",
    "geo_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "upper-eleven",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17865"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_new = list(set(geo).difference(set(bert)))\n",
    "len(geo_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-linux",
   "metadata": {},
   "source": [
    "There are 17,865 tokens that are in the new geology tokenizer and not in the original pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "capital-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df['is_new'] = [geo_df['token'][i] in list(geo_new) for i in range(0,len(geo_df['token']))]\n",
    "geo_df['sub_word'] = ['##' in str(geo_df['token'][i]) for i in range(0,len(geo_df['token']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "grand-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_option('display.max_rows')\n",
    "pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_rows', 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "removed-graduation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>token</th>\n",
       "      <th>is_new</th>\n",
       "      <th>sub_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>¯</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>¸</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312</td>\n",
       "      <td>dis</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316</td>\n",
       "      <td>##olog</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>318</td>\n",
       "      <td>##pos</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>320</td>\n",
       "      <td>canad</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>329</td>\n",
       "      <td>str</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>363</td>\n",
       "      <td>comp</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>385</td>\n",
       "      <td>##iment</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>396</td>\n",
       "      <td>geolog</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>400</td>\n",
       "      <td>wh</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>409</td>\n",
       "      <td>depos</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>410</td>\n",
       "      <td>##enti</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>416</td>\n",
       "      <td>##ediment</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>417</td>\n",
       "      <td>terr</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>432</td>\n",
       "      <td>cont</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>433</td>\n",
       "      <td>sou</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>434</td>\n",
       "      <td>rel</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>440</td>\n",
       "      <td>northw</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>446</td>\n",
       "      <td>territor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>447</td>\n",
       "      <td>grou</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>452</td>\n",
       "      <td>##duc</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>460</td>\n",
       "      <td>prov</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>464</td>\n",
       "      <td>distr</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>473</td>\n",
       "      <td>pres</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>474</td>\n",
       "      <td>hy</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>476</td>\n",
       "      <td>cor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>483</td>\n",
       "      <td>cons</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>489</td>\n",
       "      <td>gl</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>491</td>\n",
       "      <td>##erv</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index      token  is_new  sub_word\n",
       "0      83          ¯    True     False\n",
       "1      91          ¸    True     False\n",
       "2     312        dis    True     False\n",
       "3     316     ##olog    True      True\n",
       "4     318      ##pos    True      True\n",
       "5     320      canad    True     False\n",
       "6     329        str    True     False\n",
       "7     363       comp    True     False\n",
       "8     385    ##iment    True      True\n",
       "9     396     geolog    True     False\n",
       "10    400         wh    True     False\n",
       "11    409      depos    True     False\n",
       "12    410     ##enti    True      True\n",
       "13    416  ##ediment    True      True\n",
       "14    417       terr    True     False\n",
       "15    432       cont    True     False\n",
       "16    433        sou    True     False\n",
       "17    434        rel    True     False\n",
       "18    440     northw    True     False\n",
       "19    446   territor    True     False\n",
       "20    447       grou    True     False\n",
       "21    452      ##duc    True      True\n",
       "22    460       prov    True     False\n",
       "23    464      distr    True     False\n",
       "24    473       pres    True     False\n",
       "25    474         hy    True     False\n",
       "26    476        cor    True     False\n",
       "27    483       cons    True     False\n",
       "28    489         gl    True     False\n",
       "29    491      ##erv    True      True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df[geo_df['is_new']==True].reset_index().iloc[:30,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "quiet-potential",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17865\n",
      "17865\n"
     ]
    }
   ],
   "source": [
    "geo_new_ordered = list(geo_df[geo_df['is_new']==True].reset_index()['token'])\n",
    "\n",
    "# Quick sanity check on length\n",
    "print(len(geo_new_ordered))\n",
    "print(len(geo_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "clean-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df['rank_new'] = [geo_new_ordered.index(token) if token in geo_new_ordered else '-' for token in geo_df['token']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-bunch",
   "metadata": {},
   "source": [
    "Some interesting notes:\n",
    "\n",
    "- `##iment`, `##olog`, and `geolog` are in the top 10.  \n",
    "- `depos`, `##ediment`, `terr` are in the next 10.\n",
    "- `).` and `),` are both present; this is likely the effect of text with citations.  Also, it suggests that punctuation is kept together, which I hadn't thought about.\n",
    "- Other interesting tokens that have clear geographic roots: \n",
    "    - `##ict`\n",
    "    - `##uct`\n",
    "    - `glac`\n",
    "    - `##formation`\n",
    "    - `##anic`\n",
    "    - `##arbon`\n",
    "    - `##netic`\n",
    "    - `seism`\n",
    "    - `olution`\n",
    "    - `##atigraph`\n",
    "    - `##orph`\n",
    "- Three in a row, most of the way down: `##maf`, `##mafrost`, `permafrost`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-tolerance",
   "metadata": {},
   "source": [
    "There are 994 `[unusedXX]` tokens (first 999 spots, minus 5 special tokens on lines 1 and 101-104: `[PAD]`, `[UNK]`, `[CLS]`, `[SEP]`, `[MASK]`).\n",
    "\n",
    "I'm considering keeping 500 or 994 tokens.\n",
    "\n",
    "What are the top 994 geology tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cleared-contamination",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>token</th>\n",
       "      <th>is_new</th>\n",
       "      <th>sub_word</th>\n",
       "      <th>rank_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>¯</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>¸</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312</td>\n",
       "      <td>dis</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316</td>\n",
       "      <td>##olog</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>318</td>\n",
       "      <td>##pos</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>320</td>\n",
       "      <td>canad</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>329</td>\n",
       "      <td>str</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>363</td>\n",
       "      <td>comp</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>385</td>\n",
       "      <td>##iment</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>396</td>\n",
       "      <td>geolog</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>400</td>\n",
       "      <td>wh</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>409</td>\n",
       "      <td>depos</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>410</td>\n",
       "      <td>##enti</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>416</td>\n",
       "      <td>##ediment</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>417</td>\n",
       "      <td>terr</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>432</td>\n",
       "      <td>cont</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>433</td>\n",
       "      <td>sou</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>434</td>\n",
       "      <td>rel</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>440</td>\n",
       "      <td>northw</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>446</td>\n",
       "      <td>territor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>447</td>\n",
       "      <td>grou</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>452</td>\n",
       "      <td>##duc</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>460</td>\n",
       "      <td>prov</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>464</td>\n",
       "      <td>distr</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>473</td>\n",
       "      <td>pres</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>474</td>\n",
       "      <td>hy</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>476</td>\n",
       "      <td>cor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>483</td>\n",
       "      <td>cons</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>489</td>\n",
       "      <td>gl</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>491</td>\n",
       "      <td>##erv</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index      token  is_new  sub_word rank_new\n",
       "0      83          ¯    True     False        0\n",
       "1      91          ¸    True     False        1\n",
       "2     312        dis    True     False        2\n",
       "3     316     ##olog    True      True        3\n",
       "4     318      ##pos    True      True        4\n",
       "5     320      canad    True     False        5\n",
       "6     329        str    True     False        6\n",
       "7     363       comp    True     False        7\n",
       "8     385    ##iment    True      True        8\n",
       "9     396     geolog    True     False        9\n",
       "10    400         wh    True     False       10\n",
       "11    409      depos    True     False       11\n",
       "12    410     ##enti    True      True       12\n",
       "13    416  ##ediment    True      True       13\n",
       "14    417       terr    True     False       14\n",
       "15    432       cont    True     False       15\n",
       "16    433        sou    True     False       16\n",
       "17    434        rel    True     False       17\n",
       "18    440     northw    True     False       18\n",
       "19    446   territor    True     False       19\n",
       "20    447       grou    True     False       20\n",
       "21    452      ##duc    True      True       21\n",
       "22    460       prov    True     False       22\n",
       "23    464      distr    True     False       23\n",
       "24    473       pres    True     False       24\n",
       "25    474         hy    True     False       25\n",
       "26    476        cor    True     False       26\n",
       "27    483       cons    True     False       27\n",
       "28    489         gl    True     False       28\n",
       "29    491      ##erv    True      True       29"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df[geo_df['is_new']==True].reset_index(drop=False).iloc[:30,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-anniversary",
   "metadata": {},
   "source": [
    "Follow the merges for some tokens to better understand the process:\n",
    "\n",
    "- `geolog`\n",
    "- `mesozoic` +/- `paleozoic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eligible-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_merges(my_str):\n",
    "    tokens = [i for i in geo if re.sub('##', '', i) in my_str]\n",
    "    indices = [geo.index(token) for token in tokens]\n",
    "    rank_new = [geo_new_ordered.index(token) if token in geo_new_ordered else '-' for token in tokens]\n",
    "    merges = pd.DataFrame.from_dict({'index': indices, \n",
    "                                     'rank_new': rank_new, \n",
    "                                     'token': tokens})\n",
    "    merges['is_new'] = [['-', 'new'][int(token in list(geo_new_ordered))] for token in tokens]\n",
    "    merges['sub_word'] = [['-', '##'][int('##' in token)] for token in tokens]\n",
    "    return(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "corrected-secretary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_new_ordered.index('geolog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "hispanic-basic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>rank_new</th>\n",
       "      <th>token</th>\n",
       "      <th>is_new</th>\n",
       "      <th>sub_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>e</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>-</td>\n",
       "      <td>g</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>-</td>\n",
       "      <td>l</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>-</td>\n",
       "      <td>o</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>-</td>\n",
       "      <td>y</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>107</td>\n",
       "      <td>-</td>\n",
       "      <td>##o</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>109</td>\n",
       "      <td>-</td>\n",
       "      <td>##l</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>113</td>\n",
       "      <td>-</td>\n",
       "      <td>##e</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>125</td>\n",
       "      <td>-</td>\n",
       "      <td>##g</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>134</td>\n",
       "      <td>-</td>\n",
       "      <td>##y</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>227</td>\n",
       "      <td>-</td>\n",
       "      <td>##ol</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>260</td>\n",
       "      <td>-</td>\n",
       "      <td>##og</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>274</td>\n",
       "      <td>-</td>\n",
       "      <td>##ge</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>276</td>\n",
       "      <td>-</td>\n",
       "      <td>ge</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>316</td>\n",
       "      <td>3</td>\n",
       "      <td>##olog</td>\n",
       "      <td>new</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>396</td>\n",
       "      <td>9</td>\n",
       "      <td>geolog</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>621</td>\n",
       "      <td>-</td>\n",
       "      <td>geology</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>795</td>\n",
       "      <td>-</td>\n",
       "      <td>##ology</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>900</td>\n",
       "      <td>-</td>\n",
       "      <td>geo</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1752</td>\n",
       "      <td>-</td>\n",
       "      <td>log</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1785</td>\n",
       "      <td>-</td>\n",
       "      <td>##log</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2801</td>\n",
       "      <td>-</td>\n",
       "      <td>ol</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5119</td>\n",
       "      <td>-</td>\n",
       "      <td>lo</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5703</td>\n",
       "      <td>-</td>\n",
       "      <td>##lo</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8268</td>\n",
       "      <td>-</td>\n",
       "      <td>og</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10249</td>\n",
       "      <td>3996</td>\n",
       "      <td>eo</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12741</td>\n",
       "      <td>-</td>\n",
       "      <td>##gy</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16182</td>\n",
       "      <td>7462</td>\n",
       "      <td>geol</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18705</td>\n",
       "      <td>9086</td>\n",
       "      <td>gy</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>18768</td>\n",
       "      <td>9125</td>\n",
       "      <td>##geo</td>\n",
       "      <td>new</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index rank_new    token is_new sub_word\n",
       "0      46        -        e      -        -\n",
       "1      48        -        g      -        -\n",
       "2      53        -        l      -        -\n",
       "3      56        -        o      -        -\n",
       "4      66        -        y      -        -\n",
       "5     107        -      ##o      -       ##\n",
       "6     109        -      ##l      -       ##\n",
       "7     113        -      ##e      -       ##\n",
       "8     125        -      ##g      -       ##\n",
       "9     134        -      ##y      -       ##\n",
       "10    227        -     ##ol      -       ##\n",
       "11    260        -     ##og      -       ##\n",
       "12    274        -     ##ge      -       ##\n",
       "13    276        -       ge      -        -\n",
       "14    316        3   ##olog    new       ##\n",
       "15    396        9   geolog    new        -\n",
       "16    621        -  geology      -        -\n",
       "17    795        -  ##ology      -       ##\n",
       "18    900        -      geo      -        -\n",
       "19   1752        -      log      -        -\n",
       "20   1785        -    ##log      -       ##\n",
       "21   2801        -       ol      -        -\n",
       "22   5119        -       lo      -        -\n",
       "23   5703        -     ##lo      -       ##\n",
       "24   8268        -       og      -        -\n",
       "25  10249     3996       eo    new        -\n",
       "26  12741        -     ##gy      -       ##\n",
       "27  16182     7462     geol    new        -\n",
       "28  18705     9086       gy    new        -\n",
       "29  18768     9125    ##geo    new       ##"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_merges('geology')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-rabbit",
   "metadata": {},
   "source": [
    "Obervations: \n",
    "\n",
    "- Words are not necessarily built up one letter at a time. \n",
    "    - This actually makes sense; the process isn't merging single letters to multi-letter tokens, it's just merging tokens, so that can be multi-letter token + multi-letter token.\n",
    "        - E.g., `ol` + `og` = `olog`, which is new for this vocab list; `log` happens later (and `olog` doesn't happen at all), so it's not `o` + `log`. \n",
    "        - FYI, it's like this:\n",
    "            - `##ol` + `##og` = `olog`\n",
    "            - `ge` + `##olog` = `geolog`\n",
    "            - `geolog` + `##y` = `geology`\n",
    "         - The above happens before even `##ology` exists.\n",
    "    - So when thinking about branching (e.g., if something like `ozoic` is present in multiple other full words), it's not as simple as adding one letter.\n",
    "- `geology` does exist in the original vocab list even though certain sub-word tokens do not (`##olog`, `#geolog`, `geol`, etc.), so it must be built up in a different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "related-digest",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>rank_new</th>\n",
       "      <th>token</th>\n",
       "      <th>is_new</th>\n",
       "      <th>sub_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>-</td>\n",
       "      <td>c</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>e</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>-</td>\n",
       "      <td>i</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>-</td>\n",
       "      <td>m</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>-</td>\n",
       "      <td>o</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>-</td>\n",
       "      <td>s</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>67</td>\n",
       "      <td>-</td>\n",
       "      <td>z</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>106</td>\n",
       "      <td>-</td>\n",
       "      <td>##i</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>107</td>\n",
       "      <td>-</td>\n",
       "      <td>##o</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>110</td>\n",
       "      <td>-</td>\n",
       "      <td>##m</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>111</td>\n",
       "      <td>-</td>\n",
       "      <td>##s</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>113</td>\n",
       "      <td>-</td>\n",
       "      <td>##e</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>123</td>\n",
       "      <td>-</td>\n",
       "      <td>##c</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>129</td>\n",
       "      <td>-</td>\n",
       "      <td>##z</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>201</td>\n",
       "      <td>-</td>\n",
       "      <td>##es</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>214</td>\n",
       "      <td>-</td>\n",
       "      <td>##ic</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>512</td>\n",
       "      <td>-</td>\n",
       "      <td>me</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>665</td>\n",
       "      <td>-</td>\n",
       "      <td>##so</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>689</td>\n",
       "      <td>-</td>\n",
       "      <td>es</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>718</td>\n",
       "      <td>-</td>\n",
       "      <td>##oz</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>746</td>\n",
       "      <td>97</td>\n",
       "      <td>##ozo</td>\n",
       "      <td>new</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>769</td>\n",
       "      <td>101</td>\n",
       "      <td>##ozoic</td>\n",
       "      <td>new</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>775</td>\n",
       "      <td>-</td>\n",
       "      <td>so</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2069</td>\n",
       "      <td>532</td>\n",
       "      <td>mes</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2610</td>\n",
       "      <td>-</td>\n",
       "      <td>##me</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2751</td>\n",
       "      <td>-</td>\n",
       "      <td>ic</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3179</td>\n",
       "      <td>924</td>\n",
       "      <td>mesozoic</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8216</td>\n",
       "      <td>2992</td>\n",
       "      <td>meso</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11434</td>\n",
       "      <td>-</td>\n",
       "      <td>##oi</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11999</td>\n",
       "      <td>-</td>\n",
       "      <td>oz</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>13476</td>\n",
       "      <td>5794</td>\n",
       "      <td>zo</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>14207</td>\n",
       "      <td>-</td>\n",
       "      <td>##mes</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>15687</td>\n",
       "      <td>-</td>\n",
       "      <td>##zo</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>25282</td>\n",
       "      <td>13814</td>\n",
       "      <td>oi</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index rank_new     token is_new sub_word\n",
       "0      44        -         c      -        -\n",
       "1      46        -         e      -        -\n",
       "2      50        -         i      -        -\n",
       "3      54        -         m      -        -\n",
       "4      56        -         o      -        -\n",
       "5      60        -         s      -        -\n",
       "6      67        -         z      -        -\n",
       "7     106        -       ##i      -       ##\n",
       "8     107        -       ##o      -       ##\n",
       "9     110        -       ##m      -       ##\n",
       "10    111        -       ##s      -       ##\n",
       "11    113        -       ##e      -       ##\n",
       "12    123        -       ##c      -       ##\n",
       "13    129        -       ##z      -       ##\n",
       "14    201        -      ##es      -       ##\n",
       "15    214        -      ##ic      -       ##\n",
       "16    512        -        me      -        -\n",
       "17    665        -      ##so      -       ##\n",
       "18    689        -        es      -        -\n",
       "19    718        -      ##oz      -       ##\n",
       "20    746       97     ##ozo    new       ##\n",
       "21    769      101   ##ozoic    new       ##\n",
       "22    775        -        so      -        -\n",
       "23   2069      532       mes    new        -\n",
       "24   2610        -      ##me      -       ##\n",
       "25   2751        -        ic      -        -\n",
       "26   3179      924  mesozoic    new        -\n",
       "27   8216     2992      meso    new        -\n",
       "28  11434        -      ##oi      -       ##\n",
       "29  11999        -        oz      -        -\n",
       "30  13476     5794        zo    new        -\n",
       "31  14207        -     ##mes      -       ##\n",
       "32  15687        -      ##zo      -       ##\n",
       "33  25282    13814        oi    new        -"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_merges('mesozoic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "peaceful-bench",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>rank_new</th>\n",
       "      <th>token</th>\n",
       "      <th>is_new</th>\n",
       "      <th>sub_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>-</td>\n",
       "      <td>a</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>-</td>\n",
       "      <td>c</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>e</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>-</td>\n",
       "      <td>i</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>-</td>\n",
       "      <td>l</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>-</td>\n",
       "      <td>o</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57</td>\n",
       "      <td>-</td>\n",
       "      <td>p</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>67</td>\n",
       "      <td>-</td>\n",
       "      <td>z</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>106</td>\n",
       "      <td>-</td>\n",
       "      <td>##i</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>107</td>\n",
       "      <td>-</td>\n",
       "      <td>##o</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>109</td>\n",
       "      <td>-</td>\n",
       "      <td>##l</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>113</td>\n",
       "      <td>-</td>\n",
       "      <td>##e</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>122</td>\n",
       "      <td>-</td>\n",
       "      <td>##a</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>123</td>\n",
       "      <td>-</td>\n",
       "      <td>##c</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>126</td>\n",
       "      <td>-</td>\n",
       "      <td>##p</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>129</td>\n",
       "      <td>-</td>\n",
       "      <td>##z</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>204</td>\n",
       "      <td>-</td>\n",
       "      <td>##al</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>214</td>\n",
       "      <td>-</td>\n",
       "      <td>##ic</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>238</td>\n",
       "      <td>-</td>\n",
       "      <td>##le</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>278</td>\n",
       "      <td>-</td>\n",
       "      <td>al</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>455</td>\n",
       "      <td>-</td>\n",
       "      <td>##ale</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>463</td>\n",
       "      <td>-</td>\n",
       "      <td>le</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>718</td>\n",
       "      <td>-</td>\n",
       "      <td>##oz</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>746</td>\n",
       "      <td>97</td>\n",
       "      <td>##ozo</td>\n",
       "      <td>new</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>768</td>\n",
       "      <td>-</td>\n",
       "      <td>pale</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>769</td>\n",
       "      <td>101</td>\n",
       "      <td>##ozoic</td>\n",
       "      <td>new</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1106</td>\n",
       "      <td>-</td>\n",
       "      <td>pa</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1774</td>\n",
       "      <td>-</td>\n",
       "      <td>pal</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1964</td>\n",
       "      <td>499</td>\n",
       "      <td>paleozoic</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2706</td>\n",
       "      <td>756</td>\n",
       "      <td>paleo</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2751</td>\n",
       "      <td>-</td>\n",
       "      <td>ic</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4358</td>\n",
       "      <td>-</td>\n",
       "      <td>##pa</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6742</td>\n",
       "      <td>-</td>\n",
       "      <td>ale</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10249</td>\n",
       "      <td>3996</td>\n",
       "      <td>eo</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>11434</td>\n",
       "      <td>-</td>\n",
       "      <td>##oi</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>11999</td>\n",
       "      <td>-</td>\n",
       "      <td>oz</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>13476</td>\n",
       "      <td>5794</td>\n",
       "      <td>zo</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>15687</td>\n",
       "      <td>-</td>\n",
       "      <td>##zo</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>22276</td>\n",
       "      <td>-</td>\n",
       "      <td>leo</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>25282</td>\n",
       "      <td>13814</td>\n",
       "      <td>oi</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index rank_new      token is_new sub_word\n",
       "0      42        -          a      -        -\n",
       "1      44        -          c      -        -\n",
       "2      46        -          e      -        -\n",
       "3      50        -          i      -        -\n",
       "4      53        -          l      -        -\n",
       "5      56        -          o      -        -\n",
       "6      57        -          p      -        -\n",
       "7      67        -          z      -        -\n",
       "8     106        -        ##i      -       ##\n",
       "9     107        -        ##o      -       ##\n",
       "10    109        -        ##l      -       ##\n",
       "11    113        -        ##e      -       ##\n",
       "12    122        -        ##a      -       ##\n",
       "13    123        -        ##c      -       ##\n",
       "14    126        -        ##p      -       ##\n",
       "15    129        -        ##z      -       ##\n",
       "16    204        -       ##al      -       ##\n",
       "17    214        -       ##ic      -       ##\n",
       "18    238        -       ##le      -       ##\n",
       "19    278        -         al      -        -\n",
       "20    455        -      ##ale      -       ##\n",
       "21    463        -         le      -        -\n",
       "22    718        -       ##oz      -       ##\n",
       "23    746       97      ##ozo    new       ##\n",
       "24    768        -       pale      -        -\n",
       "25    769      101    ##ozoic    new       ##\n",
       "26   1106        -         pa      -        -\n",
       "27   1774        -        pal      -        -\n",
       "28   1964      499  paleozoic    new        -\n",
       "29   2706      756      paleo    new        -\n",
       "30   2751        -         ic      -        -\n",
       "31   4358        -       ##pa      -       ##\n",
       "32   6742        -        ale      -        -\n",
       "33  10249     3996         eo    new        -\n",
       "34  11434        -       ##oi      -       ##\n",
       "35  11999        -         oz      -        -\n",
       "36  13476     5794         zo    new        -\n",
       "37  15687        -       ##zo      -       ##\n",
       "38  22276        -        leo      -        -\n",
       "39  25282    13814         oi    new        -"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_merges('paleozoic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "interim-chart",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>rank_new</th>\n",
       "      <th>token</th>\n",
       "      <th>is_new</th>\n",
       "      <th>sub_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>-</td>\n",
       "      <td>a</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>-</td>\n",
       "      <td>c</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>e</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>-</td>\n",
       "      <td>i</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>-</td>\n",
       "      <td>l</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>54</td>\n",
       "      <td>-</td>\n",
       "      <td>m</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>-</td>\n",
       "      <td>o</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>57</td>\n",
       "      <td>-</td>\n",
       "      <td>p</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60</td>\n",
       "      <td>-</td>\n",
       "      <td>s</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>67</td>\n",
       "      <td>-</td>\n",
       "      <td>z</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>106</td>\n",
       "      <td>-</td>\n",
       "      <td>##i</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>107</td>\n",
       "      <td>-</td>\n",
       "      <td>##o</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>109</td>\n",
       "      <td>-</td>\n",
       "      <td>##l</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>110</td>\n",
       "      <td>-</td>\n",
       "      <td>##m</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>111</td>\n",
       "      <td>-</td>\n",
       "      <td>##s</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>113</td>\n",
       "      <td>-</td>\n",
       "      <td>##e</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>122</td>\n",
       "      <td>-</td>\n",
       "      <td>##a</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>123</td>\n",
       "      <td>-</td>\n",
       "      <td>##c</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>126</td>\n",
       "      <td>-</td>\n",
       "      <td>##p</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>129</td>\n",
       "      <td>-</td>\n",
       "      <td>##z</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>201</td>\n",
       "      <td>-</td>\n",
       "      <td>##es</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>204</td>\n",
       "      <td>-</td>\n",
       "      <td>##al</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>214</td>\n",
       "      <td>-</td>\n",
       "      <td>##ic</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>238</td>\n",
       "      <td>-</td>\n",
       "      <td>##le</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>278</td>\n",
       "      <td>-</td>\n",
       "      <td>al</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>455</td>\n",
       "      <td>-</td>\n",
       "      <td>##ale</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>463</td>\n",
       "      <td>-</td>\n",
       "      <td>le</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>512</td>\n",
       "      <td>-</td>\n",
       "      <td>me</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>665</td>\n",
       "      <td>-</td>\n",
       "      <td>##so</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>689</td>\n",
       "      <td>-</td>\n",
       "      <td>es</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>718</td>\n",
       "      <td>-</td>\n",
       "      <td>##oz</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>746</td>\n",
       "      <td>97</td>\n",
       "      <td>##ozo</td>\n",
       "      <td>new</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>768</td>\n",
       "      <td>-</td>\n",
       "      <td>pale</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>769</td>\n",
       "      <td>101</td>\n",
       "      <td>##ozoic</td>\n",
       "      <td>new</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>775</td>\n",
       "      <td>-</td>\n",
       "      <td>so</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1106</td>\n",
       "      <td>-</td>\n",
       "      <td>pa</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1774</td>\n",
       "      <td>-</td>\n",
       "      <td>pal</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1964</td>\n",
       "      <td>499</td>\n",
       "      <td>paleozoic</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2069</td>\n",
       "      <td>532</td>\n",
       "      <td>mes</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2610</td>\n",
       "      <td>-</td>\n",
       "      <td>##me</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2706</td>\n",
       "      <td>756</td>\n",
       "      <td>paleo</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2751</td>\n",
       "      <td>-</td>\n",
       "      <td>ic</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3179</td>\n",
       "      <td>924</td>\n",
       "      <td>mesozoic</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4358</td>\n",
       "      <td>-</td>\n",
       "      <td>##pa</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6742</td>\n",
       "      <td>-</td>\n",
       "      <td>ale</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8216</td>\n",
       "      <td>2992</td>\n",
       "      <td>meso</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10249</td>\n",
       "      <td>3996</td>\n",
       "      <td>eo</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>11434</td>\n",
       "      <td>-</td>\n",
       "      <td>##oi</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>11999</td>\n",
       "      <td>-</td>\n",
       "      <td>oz</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>13476</td>\n",
       "      <td>5794</td>\n",
       "      <td>zo</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>14207</td>\n",
       "      <td>-</td>\n",
       "      <td>##mes</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>15687</td>\n",
       "      <td>-</td>\n",
       "      <td>##zo</td>\n",
       "      <td>-</td>\n",
       "      <td>##</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>22276</td>\n",
       "      <td>-</td>\n",
       "      <td>leo</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>25282</td>\n",
       "      <td>13814</td>\n",
       "      <td>oi</td>\n",
       "      <td>new</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index rank_new      token is_new sub_word\n",
       "0      42        -          a      -        -\n",
       "1      44        -          c      -        -\n",
       "2      46        -          e      -        -\n",
       "3      50        -          i      -        -\n",
       "4      53        -          l      -        -\n",
       "5      54        -          m      -        -\n",
       "6      56        -          o      -        -\n",
       "7      57        -          p      -        -\n",
       "8      60        -          s      -        -\n",
       "9      67        -          z      -        -\n",
       "10    106        -        ##i      -       ##\n",
       "11    107        -        ##o      -       ##\n",
       "12    109        -        ##l      -       ##\n",
       "13    110        -        ##m      -       ##\n",
       "14    111        -        ##s      -       ##\n",
       "15    113        -        ##e      -       ##\n",
       "16    122        -        ##a      -       ##\n",
       "17    123        -        ##c      -       ##\n",
       "18    126        -        ##p      -       ##\n",
       "19    129        -        ##z      -       ##\n",
       "20    201        -       ##es      -       ##\n",
       "21    204        -       ##al      -       ##\n",
       "22    214        -       ##ic      -       ##\n",
       "23    238        -       ##le      -       ##\n",
       "24    278        -         al      -        -\n",
       "25    455        -      ##ale      -       ##\n",
       "26    463        -         le      -        -\n",
       "27    512        -         me      -        -\n",
       "28    665        -       ##so      -       ##\n",
       "29    689        -         es      -        -\n",
       "30    718        -       ##oz      -       ##\n",
       "31    746       97      ##ozo    new       ##\n",
       "32    768        -       pale      -        -\n",
       "33    769      101    ##ozoic    new       ##\n",
       "34    775        -         so      -        -\n",
       "35   1106        -         pa      -        -\n",
       "36   1774        -        pal      -        -\n",
       "37   1964      499  paleozoic    new        -\n",
       "38   2069      532        mes    new        -\n",
       "39   2610        -       ##me      -       ##\n",
       "40   2706      756      paleo    new        -\n",
       "41   2751        -         ic      -        -\n",
       "42   3179      924   mesozoic    new        -\n",
       "43   4358        -       ##pa      -       ##\n",
       "44   6742        -        ale      -        -\n",
       "45   8216     2992       meso    new        -\n",
       "46  10249     3996         eo    new        -\n",
       "47  11434        -       ##oi      -       ##\n",
       "48  11999        -         oz      -        -\n",
       "49  13476     5794         zo    new        -\n",
       "50  14207        -      ##mes      -       ##\n",
       "51  15687        -       ##zo      -       ##\n",
       "52  22276        -        leo      -        -\n",
       "53  25282    13814         oi    new        -"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_merges('paleozoic mesozoic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-button",
   "metadata": {},
   "source": [
    "---\n",
    "### Do substitutions and write vocab.txt files based on the above (cleaner tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-exclusive",
   "metadata": {},
   "source": [
    "This code is in two scripts: `download_save_pretrained_tokenizer.py` and `create_geo_tokenizers.py`.\n",
    "\n",
    "Code that writes to the operating system has been commented out to prevent making unintentional changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "infectious-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "posted-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "sophisticated-february",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bert_tokenizer.save_pretrained('/nrcan_p2/data/06_models/tokenizers/distilbert-base-uncased/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-fighter",
   "metadata": {},
   "source": [
    "The above is commented out to be sure that the files do not change, but when it was first run, it output the following:\n",
    "```\n",
    "('/nrcan_p2/data/06_models/tokenizers/distilbert-base-uncased/tokenizer_config.json',\n",
    " '/nrcan_p2/data/06_models/tokenizers/distilbert-base-uncased/special_tokens_map.json',\n",
    " '/nrcan_p2/data/06_models/tokenizers/distilbert-base-uncased/vocab.txt',\n",
    " '/nrcan_p2/data/06_models/tokenizers/distilbert-base-uncased/added_tokens.json')\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-boring",
   "metadata": {},
   "source": [
    "Read in generic bert `vocab.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "occupied-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab_file = os.path.join('/nrcan_p2/data/06_models/tokenizers/distilbert-base-uncased', \n",
    "                          'vocab.txt')\n",
    "with open(bert_vocab_file) as f:\n",
    "    temp = f.read()\n",
    "    bert = temp.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-performance",
   "metadata": {},
   "source": [
    "Read in geo tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "banner-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join('/nrcan_p2/data/06_models/tokenizers/geo_trained', \n",
    "                          'wordpiece_geo_EAIDown.xml_processed_nosentences_20210223_161710-vocab.txt')\n",
    "with open(vocab_file) as f:\n",
    "    temp = f.read()\n",
    "    geo = temp.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "elder-floating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity test\n",
    "len(bert) == len(geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-software",
   "metadata": {},
   "source": [
    "Determine which tokens are new:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "medieval-clark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17865"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_new = list(set(geo).difference(set(bert)))\n",
    "len(geo_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "referenced-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_new_ordered = [token for token in geo if token in geo_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cardiovascular-delicious",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¯',\n",
       " '¸',\n",
       " 'dis',\n",
       " '##olog',\n",
       " '##pos',\n",
       " 'canad',\n",
       " 'str',\n",
       " 'comp',\n",
       " '##iment',\n",
       " 'geolog']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_new_ordered[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-raleigh",
   "metadata": {},
   "source": [
    "Determine indices of `[unusedXX]` tokens in generic tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "demanding-market",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unused_indices = [bert.index(token) for token in bert if '[unused' in token]\n",
    "len(unused_indices) # Should be 994"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-shape",
   "metadata": {},
   "source": [
    "Function to substitute new geo tokens for `[unused]` tokens:\n",
    "\n",
    "(_Note: This will break if count >994 (greater than length of `unused_indices`); should write in a check to avoid this error._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "boxed-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subst_geo_for_unused(count):\n",
    "    assert count<=994, f'count must be less than 994'\n",
    "    vocab = bert.copy()\n",
    "    for i in range(0,count):\n",
    "        vocab[unused_indices[i]] = geo_new_ordered[i]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-profile",
   "metadata": {},
   "source": [
    "Test above function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "center-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = subst_geo_for_unused(993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "processed-tuning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['munic',\n",
       " '##istocene',\n",
       " 'strateg',\n",
       " 'arg',\n",
       " '##epend',\n",
       " 'hydroge',\n",
       " 'mineralogy',\n",
       " 'sn',\n",
       " '[unused993]',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[990:1005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cosmetic-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length = 30522\n",
      "[UNK] [UNK]\n",
      "scoti [unused195]\n",
      "[unused295] [unused295]\n",
      "paris paris\n"
     ]
    }
   ],
   "source": [
    "bert_geo_250 = subst_geo_for_unused(250)\n",
    "print(f'length = {len(bert_geo_250)}')\n",
    "print(bert_geo_250[100], bert[100])\n",
    "print(bert_geo_250[200], bert[200])\n",
    "print(bert_geo_250[300], bert[300])\n",
    "print(bert_geo_250[3000], bert[3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-criminal",
   "metadata": {},
   "source": [
    "Use above function in wrapper function that writes the new `vocab.txt` to the appropriate directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "otherwise-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_new_vocab(count):\n",
    "    bert_geo_count_vocab = subst_geo_for_unused(count)\n",
    "    with open(f'/nrcan_p2/data/06_models/tokenizers/bert_geo/bert_geo_{count}/vocab.txt', 'w') as f:\n",
    "        for i, token in enumerate(bert_geo_count_vocab):\n",
    "            if i < (len(bert_geo_count_vocab)-1):\n",
    "                f.write(f'{token}\\n')\n",
    "            else:\n",
    "                f.write(f'{token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-photograph",
   "metadata": {},
   "source": [
    "Run this for the selected token counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "representative-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_new_vocab(250)\n",
    "# write_new_vocab(500)\n",
    "# write_new_vocab(994)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-studio",
   "metadata": {},
   "source": [
    "Copy other tokenizer files appropriately. Should be made more programmatic and included in previous function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-immunology",
   "metadata": {},
   "source": [
    "`special_tokens_map.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "asian-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /nrcan_p2/data/06_models/tokenizers/distilbert-base-uncased/special_tokens_map.json \\\n",
    "# /nrcan_p2/data/06_models/tokenizers/bert_geo/bert_geo_250/special_tokens_map.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "handy-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /nrcan_p2/data/06_models/tokenizers/distilbert-base-uncased/special_tokens_map.json \\\n",
    "# /nrcan_p2/data/06_models/tokenizers/bert_geo/bert_geo_500/special_tokens_map.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-capitol",
   "metadata": {},
   "source": [
    "`tokenizer_config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "north-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /nrcan_p2/data/06_models/tokenizers/distilbert-base-uncased/tokenizer_config.json \\\n",
    "# /nrcan_p2/data/06_models/tokenizers/bert_geo/bert_geo_250/tokenizer_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "extended-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /nrcan_p2/data/06_models/tokenizers/distilbert-base-uncased/tokenizer_config.json \\\n",
    "# /nrcan_p2/data/06_models/tokenizers/bert_geo/bert_geo_500/tokenizer_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dependent-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 300\n",
    "token_dir = f'/nrcan_p2/data/06_models/tokenizers/bert_geo/bert_geo_{count}/'\n",
    "if not os.path.exists(token_dir):\n",
    "    os.makedirs(token_dir)    \n",
    "cmd = f'cp /nrcan_p2/data/06_models/tokenizers/distilbert-base-uncased/special_tokens_map.json \\\n",
    "{os.path.join(token_dir, \"special_tokens_map.json\")}'\n",
    "# os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-ministry",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing + creating examples/calculations for the report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-merchant",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exotic-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incorporate-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_250 = BertTokenizer.from_pretrained('/nrcan_p2/data/06_models/tokenizers/bert_geo/bert_geo_250')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "funded-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_500 = BertTokenizer.from_pretrained('/nrcan_p2/data/06_models/tokenizers/bert_geo/bert_geo_500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "military-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_994 = BertTokenizer.from_pretrained('/nrcan_p2/data/06_models/tokenizers/bert_geo/bert_geo_994')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-margin",
   "metadata": {},
   "source": [
    "### Test tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-rough",
   "metadata": {},
   "source": [
    "Test tokenization with original tokenizer (saved and read back in):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "western-recording",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pale',\n",
       " '##ozoic',\n",
       " 'pale',\n",
       " '##ont',\n",
       " '##ologist',\n",
       " 'me',\n",
       " '##so',\n",
       " '##ame',\n",
       " '##rica',\n",
       " 'me',\n",
       " '##so',\n",
       " '##zo',\n",
       " '##ic',\n",
       " 'america']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_250.tokenize(\"paleozoic paleontologist mesoamerica mesozoic america\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "absent-hearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paleozoic',\n",
       " 'pale',\n",
       " '##ont',\n",
       " '##ologist',\n",
       " 'me',\n",
       " '##so',\n",
       " '##ame',\n",
       " '##rica',\n",
       " 'me',\n",
       " '##so',\n",
       " '##zo',\n",
       " '##ic',\n",
       " 'america']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_500.tokenize(\"paleozoic paleontologist mesoamerica mesozoic america\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "satisfied-justice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paleozoic',\n",
       " 'paleo',\n",
       " '##nto',\n",
       " '##logist',\n",
       " 'mes',\n",
       " '##oa',\n",
       " '##meric',\n",
       " '##a',\n",
       " 'mesozoic',\n",
       " 'america']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_994.tokenize(\"paleozoic paleontologist mesoamerica mesozoic america\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-purpose",
   "metadata": {},
   "source": [
    "### Comparisons between tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-canon",
   "metadata": {},
   "source": [
    "Original tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bridal-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "smart-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenizers(mystr):\n",
    "    print(f'\\nbert_tokenizer:')\n",
    "    print(bert_tokenizer.tokenize(mystr))\n",
    "    print(f'\\nbert_geo_250:')\n",
    "    print(tokenizer_250.tokenize(mystr))\n",
    "    print(f'\\nbert_geo_500:')\n",
    "    print(tokenizer_500.tokenize(mystr))\n",
    "    print(f'\\nbert_geo_994:')\n",
    "    print(tokenizer_994.tokenize(mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "strong-immune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bert_tokenizer:\n",
      "['this', 'geology', 'sentence', 'includes', 'pale', '##oz', '##oic', ',', 'me', '##so', '##zo', '##ic', ',', 'and', 'se', '##ism', '##ology', '.']\n",
      "\n",
      "bert_geo_250:\n",
      "['this', 'geology', 'sentence', 'includes', 'pale', '##ozoic', ',', 'me', '##so', '##zo', '##ic', ',', 'and', 'seism', '##ology', '.']\n",
      "\n",
      "bert_geo_500:\n",
      "['this', 'geology', 'sentence', 'includes', 'paleozoic', ',', 'me', '##so', '##zo', '##ic', ',', 'and', 'seism', '##ology', '.']\n",
      "\n",
      "bert_geo_994:\n",
      "['this', 'geology', 'sentence', 'includes', 'paleozoic', ',', 'mesozoic', ',', 'and', 'seism', '##ology', '.']\n"
     ]
    }
   ],
   "source": [
    "compare_tokenizers('This geology sentence includes paleozoic, mesozoic, and seismology.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "controlled-posting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'es', '##press', '##o', 'machine', 'added', 'elegance', 'to', 'the', 'quo', '##ti', '##dian', 'task', 'of', 'morning', 'caf', '##fe', '##ination', '.']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.tokenize('The espresso machine added elegance to the quotidian task of morning caffeination.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-score",
   "metadata": {},
   "source": [
    "### Check proportions of subword tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "steady-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens_dict = {\n",
    "'new_tokens_250': (set(list(tokenizer_250.vocab.keys())[0:255])-set(['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])),\n",
    "'new_tokens_500': (set(list(tokenizer_500.vocab.keys())[0:505])-set(['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])),\n",
    "'new_tokens_994': (set(list(tokenizer_994.vocab.keys())[0:999])-set(['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stretch-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subword_stats(count):\n",
    "    token_list = new_tokens_dict[f'new_tokens_{count}']\n",
    "    subword_count = sum([1 for token in token_list if '##' in token])\n",
    "    non_subword_count = count - subword_count\n",
    "    subword_percent = round(subword_count/count*100, 2)\n",
    "    print(f'\\nnew geo tokens: {count}\\nsubwords: {subword_count} \\nnon-subwords: {non_subword_count}\\nsubword percentage: {subword_percent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "powered-vehicle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new geo tokens: 250\n",
      "subwords: 89 \n",
      "non-subwords: 161\n",
      "subword percentage: 35.6\n",
      "\n",
      "new geo tokens: 500\n",
      "subwords: 181 \n",
      "non-subwords: 319\n",
      "subword percentage: 36.2\n",
      "\n",
      "new geo tokens: 994\n",
      "subwords: 319 \n",
      "non-subwords: 675\n",
      "subword percentage: 32.09\n"
     ]
    }
   ],
   "source": [
    "get_subword_stats(250)\n",
    "get_subword_stats(500)\n",
    "get_subword_stats(994)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-meeting",
   "metadata": {},
   "source": [
    "For posterity:\n",
    "\n",
    "```\n",
    "new geo tokens: 250\n",
    "subwords: 89 \n",
    "non-subwords: 161\n",
    "subword percentage: 35.6\n",
    "\n",
    "new geo tokens: 500\n",
    "subwords: 181 \n",
    "non-subwords: 319\n",
    "subword percentage: 36.2\n",
    "\n",
    "new geo tokens: 994\n",
    "subwords: 319 \n",
    "non-subwords: 675\n",
    "subword percentage: 32.09\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}