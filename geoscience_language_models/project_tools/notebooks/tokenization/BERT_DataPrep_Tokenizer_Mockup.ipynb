{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep: Tokenizer Training (Mockup)\n",
    "Copyright (C) 2021 ServiceNow, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to experiment and develop around training a custom (geology-specific) tokenizer.\n",
    "\n",
    "This is likely not runnable without revisions, as the training data likely does not all still exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "* Test out tokenizer training process. \n",
    "    * Follow [ü§ó Tokenizers Quicktour] (https://huggingface.co/docs/tokenizers/python/latest/quicktour.html) to train a BPE tokenizer from scratch. This is just to check the process.\n",
    "    * Use data from Tianyi‚Äôs blind text concatenation here: `/nrcan_p2/data/03_primary/v1/all_text.txt` (used a smaller subset)\n",
    "* Create two tokenizers\n",
    "    1. Train a tokenizer from scratch for BERT, including pre- and post-processing, and WordPiece trainer, following [ü§ó Tokenization Pipeline Tutorial](https://huggingface.co/docs/tokenizers/python/latest/pipeline.html).\n",
    "        * Actually trained 2, with different amounts of data (~5 vs. 500 MB)\n",
    "    2. Create a second tokenizer that is a BERT standard tokenizer (wikipedia data subset, also uses the above pipeline)\n",
    "* Examine and compare tokenizers\n",
    "    * Pick a subset of definitely geological sentences\n",
    "    * Compare what the tokenizers do with technical terms and with common English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually check that data is where I expect it to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../../data/03_primary/v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface `tokenizers` and `transformers` should be installed in the container already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a smaller text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole dataset (`data/03_primary/v1/all_text.txt`) is 2.17 GB, and the tutorial uses a dataset with 516 MB.  Using 2.17 GB makes training the tokenizer verrrry slow.  This is why I started with a smaller subset. :) \n",
    "\n",
    "The below is just for documentation, and run from a notebook in `workspace/[SUBFOLDER]/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran this in the command line in data/03_primary/. \n",
    "# Could uncomment and add directory structure to run here.\n",
    "# ~5MB\n",
    "#!head -c 5000000 all_text.txt > short_text_5M.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~500MB\n",
    "# !head -c 500000000 ../../data/03_primary/v1/all_text.txt > ../../data/03_primary/v1/med_text_500M.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from wikipedia for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# (cd /nrcan_p2/data/01_raw/toy/wiki;\n",
    "# wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip;\n",
    "# unzip wikitext-103-raw-v1.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is at:\n",
    "\n",
    "`/nrcan_p2/data-01/raw/toy/wiki/wikitext-103-raw/`\n",
    "\n",
    "and includes: \n",
    "* `wiki.test.raw`\n",
    "* `wiki.train.raw`\n",
    "* `wiki.valid.raw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build BPE tokenizer from scratch (learn workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Tokenizer with a BPE model\n",
    "tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a trainer (BpeTrainer) for training on geological text\n",
    "# Default values: vocab_size=30000, min_frequency=0\n",
    "trainer = BpeTrainer(special_tokens=['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: May want to add to the pre-tokenizer to split on numbers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include pre-tokenizer to split inputs into words (split on whitespace)\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set location for text files (whole corpus)\n",
    "files = ['../../data/03_primary/v1/short_text_5M.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(trainer, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and reinstate with unknown token (or it won't be used)\n",
    "token_files = tokenizer.model.save(\"../../data/06_models/tokenizers/testing\", \"bpe_geo_short\")\n",
    "tokenizer.model = BPE.from_file(*token_files, unk_token=\"[UNK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as one file that contains all configuration and vocabulary\n",
    "tokenizer.save(\"../../data/06_models/tokenizers/testing/bpe_geo_short.json\")\n",
    "# Can reload with:\n",
    "# tokenizer = Tokenizer.from_file(\"../../data/06_models/tokenizers/testing/bpe_geo_short.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a quick look at the new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'ello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?']\n",
      "[45, 14735, 17, 93, 12, 221, 6, 4111, 154, 1697, 0, 36]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'geo', 'sent', 'ence', 'includes', 'pro', 'te', 'ozoic', ',', 'quartzite', ',', 'and', 'magma', '.']\n",
      "[503, 977, 9388, 374, 3726, 186, 428, 975, 17, 2822, 17, 129, 9624, 19]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"This geo sentence includes proteozoic, quartzite, and magma.\")\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build WordPiece tokenizer for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two tokenizers are trained in this section: same workflow, same parameters, different amounts of geology text (~50 MB, ~500MB). \n",
    "\n",
    "I have not done any sort of thorough assessment to determine whether the additional text makes a functional difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece # BPE\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Tokenizer with a WordPiece model\n",
    "bert_tokenizer = Tokenizer(WordPiece())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "* Unicode normalization\n",
    "* Lowercase all text\n",
    "* Remove accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on: \n",
    "* Whitespace \n",
    "* Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up for both single sequences and sequence/sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tokenizer on geology text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "* Train tokenizer on geology file(s)\n",
    "* Save model files\n",
    "* Reload and reinstate with unknown token as `[UNK]`\n",
    "* Save full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 MB text\n",
    "* This is pretty quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30522, \n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "text_files = ['/nrcan_p2/data/03_primary/metadata/metadata_nosentences_5M.txt']\n",
    "bert_tokenizer.train(trainer, text_files)\n",
    "\n",
    "# Save\n",
    "model_files = bert_tokenizer.model.save(\n",
    "    \"/nrcan_p2/data/06_models/tokenizers/testing\", \n",
    "    \"bert_geo_meta_short\"\n",
    ")\n",
    "\n",
    "# Reload with unknown token\n",
    "bert_tokenizer.model = WordPiece.from_file(*model_files, unk_token=\"[UNK]\")\n",
    "\n",
    "# Save full model\n",
    "bert_tokenizer.save(\"/nrcan_p2/data/06_models/tokenizers/testing/bert_geo_meta_short.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500 MB text\n",
    "* This take a while (8-9 minutes in a notebook in a toolkit container)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer2 = Tokenizer(WordPiece())\n",
    "bert_tokenizer2.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "bert_tokenizer2.pre_tokenizer = Whitespace()\n",
    "bert_tokenizer2.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30522, \n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "text_files = ['../../data/03_primary/v1/med_text_500M.txt']\n",
    "bert_tokenizer2.train(trainer, text_files)\n",
    "\n",
    "# Save\n",
    "model_files = bert_tokenizer2.model.save(\n",
    "    \"../../data/06_models/tokenizers/testing\", \n",
    "    \"bert-geo-med\"\n",
    ")\n",
    "\n",
    "# Reload with unknown token\n",
    "bert_tokenizer2.model = WordPiece.from_file(*model_files, unk_token=\"[UNK]\")\n",
    "\n",
    "# Save full model\n",
    "bert_tokenizer2.save(\"../../data/06_models/tokenizers/testing/bert_geo_med.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at new BERT tokenizer(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First tokenizer (small):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hell', '##o', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "[1, 18884, 111, 17, 67, 12, 464, 6, 721, 214, 1105, 0, 36, 2]\n"
     ]
    }
   ],
   "source": [
    "output = bert_tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'geo', 'sent', '##ence', 'includes', 'prote', '##ozoic', ',', 'quartzite', ',', 'and', 'magma', '.', '[SEP]']\n",
      "[1, 308, 323, 9829, 420, 3773, 19496, 1093, 17, 2558, 17, 184, 9132, 19, 2]\n"
     ]
    }
   ],
   "source": [
    "output = bert_tokenizer.encode(\"This geo sentence includes proteozoic, quartzite, and magma.\")\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second tokenizer (medium):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'geo', 'sent', '##ence', 'includes', 'prote', '##ozoic', ',', 'quartzite', ',', 'and', 'magma', '.', '[SEP]']\n",
      "[1, 1042, 2005, 5599, 1149, 3384, 20157, 1849, 39, 2296, 39, 892, 4649, 41, 2]\n"
     ]
    }
   ],
   "source": [
    "output = bert_tokenizer2.encode(\"This geo sentence includes proteozoic, quartzite, and magma.\")\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick wiki tokenizer for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a little while, but I'm not sure how long, maybe around 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tokenizer = Tokenizer(WordPiece())\n",
    "wiki_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "wiki_tokenizer.pre_tokenizer = Whitespace()\n",
    "wiki_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train - same trainer as above, with different text files\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30522, \n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "text_files_wiki = [f\"/nrcan_p2/data/01_raw/toy/wiki/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "wiki_tokenizer.train(trainer, text_files_wiki)\n",
    "\n",
    "# Save\n",
    "model_files_wiki = wiki_tokenizer.model.save(\n",
    "    \"/nrcan_p2/data/06_models/tokenizers/testing\", \n",
    "    \"bert-wiki\"\n",
    ")\n",
    "\n",
    "# Reload with unknown token\n",
    "wiki_tokenizer.model = WordPiece.from_file(*model_files_wiki, unk_token=\"[UNK]\")\n",
    "\n",
    "# Save full model\n",
    "wiki_tokenizer.save(\"/nrcan_p2/data/06_models/tokenizers/testing/bert_wiki.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare tokenizers manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminarily, this suggests that using geological data to train a tokenizer might be a useful part of our experimental pipeline.  I.e., it does affect the way that geological terms are tokenized; much more experimentation would be necessary to determine whether or not this has an effect on downstream task accuracy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on wikipedia:\n",
      "['[CLS]', 'this', 'ge', '##o', 'sentence', 'includes', 'prote', '##ozo', '##ic', ',', 'quart', '##zi', '##te', ',', 'and', 'magma', '.', '[SEP]']\n",
      "\n",
      "Trained on 5MB geo data:\n",
      "['[CLS]', 'this', 'geo', 'sent', '##ence', 'includes', 'prote', '##ozoic', ',', 'quartzite', ',', 'and', 'magma', '.', '[SEP]']\n",
      "\n",
      "Trained on 500MB geo data:\n",
      "['[CLS]', 'this', 'geo', 'sent', '##ence', 'includes', 'prote', '##ozoic', ',', 'quartzite', ',', 'and', 'magma', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "geo_sentence = \"This geo sentence includes proteozoic, quartzite, and magma.\"\n",
    "print(\"Trained on wikipedia:\")\n",
    "print(wiki_tokenizer.encode(geo_sentence).tokens)\n",
    "print(\"\")\n",
    "print(\"Trained on 5MB geo data:\")\n",
    "print(bert_tokenizer.encode(geo_sentence).tokens)\n",
    "print(\"\")\n",
    "print(\"Trained on 500MB geo data:\")\n",
    "print(bert_tokenizer2.encode(geo_sentence).tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}